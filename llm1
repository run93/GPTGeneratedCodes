Over time, these ontologies can become unwieldy and difficult to maintain, especially when attributes are stored as flat, unstructured lists. In this project, we are working with an ontology that contains over 8,000 attributes, currently lacking a coherent knowledge graph structure that defines entities, their properties, and interrelationships. This flat model creates challenges in consistency, discoverability, and semantic clarity, especially when integrating new attributes or aligning data across departments. A key issue is the ambiguity between what constitutes a distinct attribute versus what should be treated as a feature or component of a broader attribute (e.g., Address Line 1, City, and Zip Code as parts of Address). Without a standardized approach to attribute granularity, semantic drift and redundancy quickly accumulate. A well-structured knowledge graph is essential not only for effective ontology governance and integration, but also for powering downstream applications such as search, data linking, and automated reasoning. As organizations scale their data infrastructure, the ability to intelligently organize and standardize ontology elements becomes a critical capability—one that this project aims to deliver with the help of large language models (LLMs).

Technical Scope and Modular LLM Architecture
This project adopts a modular LLM-based architecture to automate two core ontology engineering tasks: taxonomy construction and attribute standardization. Rather than relying on a single end-to-end LLM call, the system is divided into multiple functional modules, each designed to carry out a specialized task—such as concept classification, relationship inference, name normalization, and ontology export. These modules are orchestrated to work together using a shared LLM backbone (e.g., GPT-4), each powered by domain-specific prompts and logic. First, a taxonomy builder module analyzes the existing attributes to infer class hierarchies, group related terms under parent categories, and identify sub-attributes. Next, a classification module handles incoming new elements, determining whether each is a new attribute, a synonym of an existing one, or a sub-feature. A normalization module ensures consistency in naming conventions, while an export module converts the structured output into OWL or RDF format. This modular approach increases flexibility, interpretability, and scalability, allowing the ontology to evolve intelligently as new data becomes available.
